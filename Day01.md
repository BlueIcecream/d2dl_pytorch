# Day01

## 3.1 线性回归

### 练习

#### 选择题2

```python
def squared_loss(y_hat, y):
	return (y_hat - y.view(y_hat.size())) ** 2 / 2
```

y_hat的形状是[n, 1]，而y的形状是[n]，两者相减得到的结果的形状是[n, n]，相当于用y_hat的每一个元素分别减去y的所有元素，所以无法得到正确的损失值。对于第一个选项，y_hat.view(-1)的形状是[n]，与y一致，可以相减；对于第二个选项，y.view(-1)的形状仍是[n]，所以没有解决问题；对于第三个选项和第四个选项，y.view(y_hat.shape)和y.view(-1, 1)的形状都是[n, 1]，与y_hat一致，可以相减。以下是一段示例代码：

```python
x = torch.arange(3)
y = torch.arange(3).view(3, 1)
print(x)
print(y)
print(x + y)
```

#### 填空题1

```python
yy_hat = torch.tensor( (2.33, 1.07, 1.23) )
yy = torch.tensor( (3.14, 0.98, 1.32) )
mse = (yy_hat - yy.view(yy_hat.size())) ** 2 / 2
print(mse.mean())

tensor(0.1121)
```

## 3.4 softmax回归

训练集上的准确率是在一个epoch的过程中计算得到的，测试集上的准确率是在一个epoch结束后计算得到的，后者的模型参数更优

## 3.8 多层感知机

## 10.1 文本预处理

## 10.2 语言模型

### 练习

#### 选择题4

训练数据中总共有11个样本，而批量大小为2，所以数据集会被拆分成2段，每段包含5个样本：[0, 1, 2, 3, 4]和[5, 6, 7, 8, 9]，而时间步数为2，所以第二个批量为[2, 3]和[7, 8]

## 10.3 循环神经网络基础

采用随机采样需要在每个小批量更新前初始化隐藏状态是因为每个样本包含完整的时间序列信息